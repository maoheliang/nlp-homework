import os
import pandas as pd
import pdfplumber
from docx import Document
from pptx import Presentation
from datetime import datetime
import pytesseract
from openpyxl import load_workbook
from docx.oxml.ns import qn
from imports import *


# Optional: for .doc to .docx conversion
import platform
if platform.system() == "Windows":
    import win32com.client
elif platform.system() in ["Linux", "Darwin"]:
    import subprocess

class IncrementalDocumentRetriever:
    def __init__(self, model_name="./models/bge-small-zh-v1.5", space="cosine", ef_construction=200, M=16, device="cpu"):
        self.model = SentenceTransformer(model_name, device=device)
        self.space = space
        self.ef_construction = ef_construction
        self.M = M
        self.index = None
        self.dim = None
        self.documents = []  # list of texts
        self.doc_ids = []    # list of unique int IDs
        self.id_counter = 0  # next doc id

    def _preprocess(self, text):
        return "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š" + text

    def _get_embedding(self, text):
        processed = self._preprocess(text)
        return self.model.encode([processed], normalize_embeddings=True)[0]

    def _init_index(self, dim):
        self.index = hnswlib.Index(space=self.space, dim=dim)
        self.index.init_index(max_elements=10000, ef_construction=self.ef_construction, M=self.M)
        self.index.set_ef(50)

    def add_document(self, text: str):
        embedding = self._get_embedding(text)
        if self.index is None:
            self.dim = embedding.shape[0]
            self._init_index(self.dim)

        doc_id = self.id_counter
        self.index.add_items([embedding], [doc_id])
        self.documents.append(text)
        self.doc_ids.append(doc_id)
        self.id_counter += 1

    def search(self, query, top_k=5, return_scores=False) -> Union[List[Tuple[str, float]], str]:
        if self.index is None or self.id_counter == 0:
            raise ValueError("No documents indexed yet.")
        query_vec = self._get_embedding(query)
        labels, distances = self.index.knn_query(query_vec, k=min(top_k, self.id_counter))
        results = []
        for idx, dist in zip(labels[0], distances[0]):
            text = self.documents[self.doc_ids.index(idx)]
            score = 1 - dist
            results.append((text, score))
        if return_scores:
            return results
        else:
            return "\n".join([text for text, _ in results])
    
    def search_by_threshold(self, query: str, threshold: float = 0.3, return_scores: bool = False) -> Union[List[Tuple[str, float]], str]:
        if self.index is None or self.id_counter == 0:
            raise ValueError("No documents indexed yet.")
        
        query_vec = self._get_embedding(query)
        # brute-force æ‰€æœ‰æ–‡æ¡£ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ¯”ç”¨ index å¿«ä¸äº†å¤ªå¤šï¼Œå› ä¸º hnswlib ä¸æ”¯æŒ range queryï¼‰
        similarities = []
        for idx, doc_id in enumerate(self.doc_ids):
            doc_vec = self.index.get_items([doc_id])[0]
            score = np.dot(query_vec, doc_vec)  # å› ä¸ºæ˜¯ normalize åçš„å‘é‡
            if score >= threshold:
                similarities.append((self.documents[idx], score))
        
        # æŒ‰ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        if return_scores:
            return similarities
        else:
            return "\n".join([text for text, _ in similarities])

    def save(self, index_path: str, meta_path: str):
        if self.index:
            self.index.save_index(index_path)
        meta = {
            "documents": self.documents,
            "doc_ids": self.doc_ids,
            "id_counter": self.id_counter,
            "dim": self.dim
        }
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

    def load(self, index_path: str, meta_path: str):
        if not os.path.exists(index_path) or not os.path.exists(meta_path):
            raise FileNotFoundError("Index or metadata file not found.")

        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
            self.documents = meta["documents"]
            self.doc_ids = meta["doc_ids"]
            self.id_counter = meta["id_counter"]
            self.dim = meta["dim"]

        self.index = hnswlib.Index(space=self.space, dim=self.dim)
        self.index.load_index(index_path)
        self.index.set_ef(50)

class DocumentProcessor:
    """
    å¤„ç†æ–‡æ¡£çš„ç±»
    """
    def __init__(self, use_retriever=False, model_name="./models/bge-small-zh-v1.5", device="cpu"):
        os.makedirs("./data", exist_ok=True)
        self.data = pd.DataFrame(columns=[
            "filename", "filetype", "content", "content_type", 
            "order_index", "timestamp"
        ])
        if use_retriever:
            self.retriever = IncrementalDocumentRetriever(model_name=model_name, device=device)

    def process_file(self, filepath):
        """
        å¤„ç†æ–‡ä»¶
        :param filepath: æ–‡ä»¶è·¯å¾„
        """
        # è·å–æ–‡ä»¶æ‰©å±•å
        ext = os.path.splitext(filepath)[1].lower()

        # å¦‚æœæ–‡ä»¶æ‰©å±•åä¸º.docï¼Œåˆ™å°†å…¶è½¬æ¢ä¸º.docxæ ¼å¼
        if ext == '.doc':
            filepath = self._convert_doc_to_docx(filepath)
            ext = '.docx'

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¤„ç†æ–¹æ³•
        method = {
            ".pdf": self._process_pdf,
            ".docx": self._process_docx,
            ".pptx": self._process_pptx,
            ".xlsx": self._process_excel,
            ".csv": self._process_csv
        }.get(ext)

        # å¦‚æœæ–‡ä»¶æ‰©å±•åä¸åœ¨æ”¯æŒèŒƒå›´å†…ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if not method:
            raise ValueError(f"Unsupported file type: {ext}")

        # è°ƒç”¨å¤„ç†æ–¹æ³•ï¼Œè·å–æ–‡ä»¶å†…å®¹
        contents = method(filepath)
        # å°†æ–‡ä»¶è·¯å¾„ã€æ‰©å±•åå’Œå†…å®¹æ·»åŠ åˆ°å†å²è®°å½•ä¸­
        self._append_to_data(filepath, ext, contents)

    def _append_to_data(self, filepath, filetype, contents):
        """
         å°†æ–‡ä»¶è·¯å¾„ã€æ‰©å±•åå’Œå†…å®¹æ·»åŠ åˆ°å†å²è®°å½•ä¸­
        :param filepath: æ–‡ä»¶è·¯å¾„
        :param filetype: æ–‡ä»¶æ‰©å±•å
        :param contents: æ–‡ä»¶å†…å®¹
        """
        # è·å–å½“å‰æ—¶é—´æˆ³
        now = datetime.now().isoformat()
        # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨è®°å½•
        records = []
        # éå†contentsä¸­çš„æ¯ä¸ªå…ƒç´ 
        for order_index, content_type, content in contents:
            # å¦‚æœcontentä¸ä¸ºç©ºï¼Œåˆ™å°†è®°å½•æ·»åŠ åˆ°recordsåˆ—è¡¨ä¸­
            if content.strip():
                records.append({
                    "filename": os.path.abspath(os.path.normpath(filepath)),
                    "filetype": filetype,
                    "content": content.strip(),
                    "content_type": content_type,
                    "order_index": order_index,
                    "timestamp": now
                })
        # å¦‚æœrecordsåˆ—è¡¨ä¸ä¸ºç©ºï¼Œåˆ™å°†recordsè½¬æ¢ä¸ºDataFrameï¼Œå¹¶æ·»åŠ åˆ°self.dataä¸­
        if records:
            new_records = pd.DataFrame(records)
            self.data = pd.concat([self.data, new_records], ignore_index=True)
            if hasattr(self, 'retriever'):
                record_str=f"====æ–‡ä»¶åï¼š{filepath}====\n\n"
                for record in records:
                    record_str+=f"ä¿¡æ¯ç±»å‹ï¼š{record['content_type']}\n\n"
                    record_str+=f"å†…å®¹ï¼š{record['content']}\n"
                # print(record_str)
                self.retriever.add_document(record_str)

    def _process_pdf(self, filepath):
        """
         å¤„ç†pdfæ–‡ä»¶
        """
        def clean_line_breaks(text: str) -> str:
            # å°†éå¥æœ«æ ‡ç‚¹åçš„æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
            pattern = r'(?<![ã€‚ï¼ï¼Ÿï¼›ï¼š)â€ã€‹ï¼‰\]ï¼‰])\n(?!\n)'  # æ¢è¡Œå‰ä¸æ˜¯ç»“å°¾ç¬¦ï¼Œä¸”åé¢ä¸æ˜¯æ®µè½ç©ºè¡Œ
            return re.sub(pattern, '', text)
        contents = []
        full_text = []
        order = 1  # è¡¨æ ¼ä» 1 å¼€å§‹ï¼Œæ–‡æœ¬å ç”¨ order 0

        with pdfplumber.open(filepath) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    full_text.append(text)
                else:
                    try:
                        image = page.to_image(resolution=300).original
                        ocr_text = pytesseract.image_to_string(image)
                        full_text.append(ocr_text)
                    except Exception:
                        pass

                for table in page.extract_tables():
                    df = pd.DataFrame(table[1:], columns=table[0]) if len(table) > 1 else pd.DataFrame(table)
                    df = df.fillna("ç©ºå€¼")
                    contents.append((order, 'table', df.to_csv(index=False)))
                    order += 1

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬é¡µä¸ºä¸€æ¡å†…å®¹
        if full_text:
            merged_text = "\n".join(full_text)
            merged_text = clean_line_breaks(merged_text)
            contents.insert(0, (0, 'text', merged_text))

        return contents


    def _is_numbered_paragraph(self, paragraph):
        """
         åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºç¼–å·æ®µ
        """
        pPr = paragraph._p.find(qn('w:pPr'))
        if pPr is not None:
            numPr = pPr.find(qn('w:numPr'))
            return numPr is not None
        return False

    def _process_docx(self, filepath):
        """
         å¤„ç†docxæ–‡ä»¶
        """
        doc = Document(filepath)
        contents = []
        order = 0

        paragraphs = []
        for para in doc.paragraphs:
            text = para.text.strip()
            if text:
                if self._is_numbered_paragraph(para):
                    text = "[ç¼–å·æ®µ] " + text
                paragraphs.append(text)

        full_text = "\n".join(paragraphs)
        if full_text:
            contents.append((order, 'text', full_text))
            order += 1

        for table in doc.tables:
            data = [[cell.text if cell.text.strip() else "ç©ºå€¼" for cell in row.cells] for row in table.rows]
            if data:
                header = data[0]
                rows = data[1:]
                df = pd.DataFrame(rows, columns=header)
            else:
                df = pd.DataFrame()
            contents.append((order, 'table', df.to_csv(index=False)))
            order += 1
        return contents

    def _process_pptx(self, filepath):
        """
         å¤„ç†pptxæ–‡ä»¶
        """
        # æ‰“å¼€æŒ‡å®šè·¯å¾„çš„pptxæ–‡ä»¶
        prs = Presentation(filepath)
        # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨pptxæ–‡ä»¶ä¸­çš„å†…å®¹
        contents = []
        # åˆå§‹åŒ–ä¸€ä¸ªå˜é‡ï¼Œç”¨äºè®°å½•å†…å®¹çš„é¡ºåº
        order = 0
        # éå†pptxæ–‡ä»¶ä¸­çš„æ¯ä¸€å¼ å¹»ç¯ç‰‡
        for slide in prs.slides:
            # éå†å¹»ç¯ç‰‡ä¸­çš„æ¯ä¸€ä¸ªå½¢çŠ¶
            for shape in slide.shapes:
                # å¦‚æœå½¢çŠ¶æœ‰æ–‡æœ¬å±æ€§ï¼Œå¹¶ä¸”æ–‡æœ¬ä¸ä¸ºç©º
                if hasattr(shape, "text") and shape.text.strip():
                    # å°†å†…å®¹çš„é¡ºåºã€ç±»å‹å’Œæ–‡æœ¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                    contents.append((order, 'text', shape.text))
                    # é¡ºåºåŠ 1
                    order += 1
        # è¿”å›å­˜å‚¨å†…å®¹çš„åˆ—è¡¨
        return contents
    
    def _process_excel(self, filepath):
        """
         å¤„ç†excelæ–‡ä»¶
        """
        from openpyxl import load_workbook
        wb = load_workbook(filepath, data_only=False)
        contents = []
        order = 0
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            rows = []
            for row in sheet.iter_rows(values_only=False):
                row_values = []
                for cell in row:
                    val = cell.value
                    if val is not None:
                        if cell.number_format and isinstance(val, (float, int)):
                            if '%' in cell.number_format:
                                val = f"{val:.2%}"
                            else:
                                val = str(val)
                        else:
                            val = str(val)
                    else:
                        val = "ç©ºå€¼"
                    row_values.append(val)
                rows.append(row_values)
            if rows:
                header = rows[0]
                data = rows[1:]
                df = pd.DataFrame(data, columns=header)
            else:
                df = pd.DataFrame()
            df = df.fillna("ç©ºå€¼")
            # csv_data = df.to_csv(index=False).replace('_x000D_', '\n')
            csv_data = df.to_csv(index=False)
            sheet_block = f"Sheet: {sheet_name}\n{csv_data}"
            contents.append((order, 'table', sheet_block))
            order += 1
        return contents

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†CSVæ–‡ä»¶
    def _process_csv(self, filepath):
        """
         å¤„ç†CSVæ–‡ä»¶
        """
        # ä½¿ç”¨pandasåº“è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(filepath)
        # è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«0ã€'table'å’Œdf.to_csv(index=False)çš„å€¼
        return [(0, 'table', df.to_csv(index=False))]

    def _convert_doc_to_docx(self, doc_path):
        """
         å°†docæ–‡ä»¶è½¬æ¢ä¸ºdocxæ–‡ä»¶
        """
        # åˆ¤æ–­æ“ä½œç³»ç»Ÿç±»å‹
        if platform.system() == "Windows":
            # ä½¿ç”¨win32comåº“è°ƒç”¨Wordåº”ç”¨ç¨‹åº
            word = win32com.client.Dispatch("Word.Application")
            # æ‰“å¼€æŒ‡å®šè·¯å¾„çš„docæ–‡ä»¶
            doc = word.Documents.Open(doc_path)
            # å®šä¹‰æ–°çš„æ–‡ä»¶è·¯å¾„ï¼Œæ–‡ä»¶æ ¼å¼ä¸ºdocx
            new_path = doc_path + "x"  # .docx
            # å°†docæ–‡ä»¶å¦å­˜ä¸ºdocxæ–‡ä»¶
            doc.SaveAs(new_path, FileFormat=16)  # wdFormatDocumentDefault
            # å…³é—­docæ–‡ä»¶
            doc.Close()
            # é€€å‡ºWordåº”ç”¨ç¨‹åº
            word.Quit()
            # è¿”å›æ–°çš„æ–‡ä»¶è·¯å¾„
            return new_path
        elif platform.system() in ["Linux", "Darwin"]:
            # å®šä¹‰æ–°çš„æ–‡ä»¶è·¯å¾„ï¼Œæ–‡ä»¶æ ¼å¼ä¸ºdocx
            new_path = doc_path + "x"
            # ä½¿ç”¨subprocessåº“è°ƒç”¨libreofficeå‘½ä»¤è¡Œå·¥å…·å°†docæ–‡ä»¶è½¬æ¢ä¸ºdocxæ–‡ä»¶
            try:
                subprocess.run(["libreoffice", "--headless", "--convert-to", "docx", "--outdir", os.path.dirname(doc_path), doc_path])
                # è¿”å›æ–°çš„æ–‡ä»¶è·¯å¾„
                return new_path
            except subprocess.CalledProcessError as e:
                print(f"Conversion failed: {e}")
        else:
            # æŠ›å‡ºå¼‚å¸¸ï¼Œä¸æ”¯æŒå½“å‰æ“ä½œç³»ç»Ÿ
            raise EnvironmentError("Unsupported OS for .doc conversion")

    # è·å–å†å²è®°å½•
    def get_data(self)-> pd.DataFrame:
        # è¿”å›å†å²è®°å½•çš„å‰¯æœ¬
        return deepcopy(self.data)
    
    def get_str_data(self)-> str:
        """
        å°†æ•°æ®è½¬åŒ–ä¸ºå­—ç¬¦ä¸²è¡¨ç¤ºè¾“å‡º
        """
        grouped = self.data.groupby("filename")
        str_data = ""
        for filename, group in grouped:
            str_data += f"\n=== ğŸ“„ æ–‡ä»¶: {filename}===\n\n"
            for _, row in group.iterrows():
                str_data += f"ä¿¡æ¯ç±»å‹: {row['filetype']}\n\n"   
                str_data += f"å†…å®¹: {row['content']}\n"
        return str_data


    def export_data(self, out_file="./data/report.xlsx"):
        """
         å¯¼å‡ºå†å²è®°å½•åˆ°Excelæ–‡ä»¶
         out_file: å¯¼å‡ºçš„Excelæ–‡ä»¶è·¯å¾„
        """
        # å¯¼å…¥xlsxwriteråº“
        import xlsxwriter

        df = self.data.copy()

        # æ¸…ç†å†…å®¹ä¸­çš„å¼‚å¸¸å›è½¦å­—ç¬¦ï¼Œç»Ÿä¸€ä¸º \n
        df['content'] = df['content'].astype(str)
        df['content'] = df['content'].str.replace('\r\n', '\n', regex=False)
        df['content'] = df['content'].str.replace('\r', '\n', regex=False)
        df['content'] = df['content'].str.replace('_x000D_', '\n', regex=False)

        with pd.ExcelWriter(out_file, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='Report', startrow=1, header=False)

            workbook  = writer.book
            worksheet = writer.sheets['Report']

            # å†™å…¥è¡¨å¤´
            for col_num, value in enumerate(df.columns):
                worksheet.write(0, col_num, value)

            # è‡ªåŠ¨æ¢è¡Œæ ¼å¼
            wrap_format = workbook.add_format({'text_wrap': True, 'valign': 'top'})

            # ä¸ºæ¯ä¸ªå•å…ƒæ ¼è®¾ç½®æ¢è¡Œæ ¼å¼
            for row_num in range(1, len(df) + 1):
                for col_num in range(len(df.columns)):
                    worksheet.write(row_num, col_num, df.iat[row_num - 1, col_num], wrap_format)

            # è®¾ç½®åˆ—å®½ï¼ˆå¯è°ƒï¼‰
            worksheet.set_column(0, len(df.columns) - 1, width=40)

    def print_data_summary(self, max_lines=-1):
        """
        æ‰“å°å†å²è®°å½•çš„æ‘˜è¦
        max_lines: æ¯ä¸ªå†…å®¹é¢„è§ˆçš„æœ€å¤§è¡Œæ•°ï¼Œ-1è¡¨ç¤ºä¸é™åˆ¶
        """
        # æ ¹æ®æ–‡ä»¶åå¯¹å†å²è®°å½•è¿›è¡Œåˆ†ç»„
        grouped = self.data.groupby("filename")
        # éå†åˆ†ç»„åçš„å†å²è®°å½•
        for filename, group in grouped:
            # æ‰“å°æ–‡ä»¶åå’Œæ–‡ä»¶ç±»å‹
            print(f"\n=== ğŸ“„ æ–‡ä»¶: {filename}ï¼ˆ{group.iloc[0]['filetype']}ï¼‰===")
            # éå†æ¯ä¸ªæ–‡ä»¶çš„å†å²è®°å½•
            for _, row in group.iterrows():
                # å¦‚æœmax_lineså¤§äº0ï¼Œåˆ™æˆªå–å†…å®¹çš„å‰max_linesè¡Œ
                if max_lines > 0:
                    content_preview = row['content'].strip().splitlines()[:max_lines]
                # å¦åˆ™ï¼Œæˆªå–å…¨éƒ¨å†…å®¹
                else:
                    content_preview = row['content'].strip().splitlines()
                # å°†å†…å®¹é¢„è§ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                preview_str = "\n    ".join(content_preview)
                # æ‰“å°å†…å®¹ç±»å‹ã€é¡ºåºå’Œæ—¶é—´
                print(f"  â–¶ ç±»å‹: {row['content_type']}, é¡ºåº: {row['order_index']}, æ—¶é—´: {row['timestamp']}")
                # æ‰“å°å†…å®¹é¢„è§ˆ
                print(f"    å†…å®¹é¢„è§ˆ:\n    {preview_str}")



if __name__ == "__main__":
    processor = DocumentProcessor(use_retriever=True,device="cpu")
    processor.process_file("./data/2025å¹´5æœˆæŠ¤ç†éƒ¨ç†è®ºçŸ¥è¯†åŸ¹è®­.docx")
    processor.process_file("./data/2025å¹´5æœˆæ‰‹å«ç”Ÿæ‰§è¡Œä¸“é¡¹åŸ¹è®­ä¸è¯„ä¼°æ€»ç»“.pdf")
    processor.process_file("./data/æ‰‹å«ç”ŸåŸ¹è®­å„ç§‘å®¤å‚ä¸ä¸è€ƒæ ¸æƒ…å†µç»Ÿè®¡.xlsx")
    # processor.export_data()

    # processor.print_data_summary()